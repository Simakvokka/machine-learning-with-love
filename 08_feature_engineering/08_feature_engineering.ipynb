{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в машинное обучение\n",
    "\n",
    "## Семинар #8\n",
    "\n",
    "### Екатерина Кондратьева\n",
    "\n",
    "ekaterina.kondrateva@skoltech.ru\n",
    "\n",
    "## Отбор и генерация признаков (Feature Engineering). Поиск и оптимизация модели (Grid Search). Поиск аномалий (Anomaly Detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Отбор и генерация признаков: Feature Engineering\n",
    "\n",
    "\n",
    "Feature Engineering, как мы уже знаем из предыдущей лекции - очень общий термин, который включает в себя:\n",
    "    a. преобработку данных и составление датасета\n",
    "    - перевод категориальных признаков в бинарные\n",
    "    - заполнение пропусков данных\n",
    "    - снижение размерности данных (выбор характеристик)\n",
    "    - генерацию новых признаков из набора данных. включает: генерацию новых характеристик исходя из знания предметной области, или геометрические методы снижения размерности данных\n",
    "    \n",
    "### Источники:\n",
    "    \n",
    "   1. Размышления на тему https://habr.com/ru/company/mlclass/blog/248129/,\n",
    "    https://habr.com/ru/company/mlclass/blog/249759/\n",
    "   2. Для датасета Титаник https://habr.com/ru/company/otus/blog/433084/\n",
    "   3. Лекция https://www.youtube.com/watch?v=leTyvBPhYzw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример 1: Выжившие в катастрофе Титаника"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соревнование: \n",
    "Источник: https://www.kaggle.com/kernels/scriptcontent/13445201/download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('feature_engineering_for_titanic/')\n",
    "%run -i titanic_solved.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример 2: Крестики - нолики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конкурс: https://datahub.io/machine-learning/tic-tac-toe-endgame\n",
    "\n",
    "\n",
    "Подходы к решению: https://towardsdatascience.com/tic-tac-toe-creating-unbeatable-ai-with-minimax-algorithm-8af9e52c1e7d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from mlxtend.plotting import plot_decision_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/tic-tac-toe_csv.csv')\n",
    "X, y = data.drop('class', axis=1), data['class'].astype(int)\n",
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.applymap(lambda v: 1 if v == 'x' else -1 if v == 'o' else 0)\n",
    "X.sample(4).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=400, max_depth=10)\n",
    "svm = SVC(gamma='auto')\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "clfs = (rf, svm, lr)\n",
    "\n",
    "for clf in clfs:\n",
    "    score = cross_val_score(clf, X, y, cv=5).mean()\n",
    "    name = clf.__class__.__name__\n",
    "    print(f\"{name} scored {round(score, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Оптимизация модели: поиск гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'kernel':( 'linear', 'rbf', 'poly', 'sigmoid'), \n",
    "    'C':[0.1, 0.2, 0.21, 0.23, 0.25, 0.3, 0.4, 0.5, 1,  5]\n",
    "}\n",
    "svm_new = GridSearchCV(svm, parameters, cv=5)\n",
    "svm_new.fit(X, y)\n",
    "\n",
    "print(f\"Best score for SVM {svm_new.best_score_}\")\n",
    "print(f\"Best params for SVM {svm_new.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос**: по какому критерию происходит выбор модели и гиперпараметров?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=400, max_depth=10, random_state=404)\n",
    "\n",
    "X_new = X\n",
    "X_new['sum'] = X.sum(1)\n",
    "score = cross_val_score(rf, X_new, y, cv=5).mean()\n",
    "name = rf.__class__.__name__\n",
    "\n",
    "print(f\"{name} scored {round(score, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = pd.DataFrame()\n",
    "X_features['rowT'] = X['TL'] + X['TM'] + X['TR']\n",
    "X_features['rowM'] = X['ML'] + X['MM'] + X['MR']\n",
    "X_features['rowB'] = X['BL'] + X['BM'] + X['BR']\n",
    "X_features['colL'] = X['TL'] + X['ML'] + X['BL']\n",
    "X_features['colM'] = X['TM'] + X['MM'] + X['BM']\n",
    "X_features['colR'] = X['TR'] + X['MR'] + X['BR']\n",
    "X_features['diag1'] = X['TL'] + X['MM'] + X['BR']\n",
    "X_features['diag2'] = X['BL'] + X['MM'] + X['TR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=400, max_depth=10, random_state=404)\n",
    "\n",
    "score = cross_val_score(rf, X_features, y, cv=5).mean()\n",
    "name = rf.__class__.__name__\n",
    "print(f'{name} scored {round(score, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'kernel':( 'linear', 'rbf', 'poly', 'sigmoid'), \n",
    "    'C':[0.1, 0.2, 0.21, 0.23, 0.25, 0.3, 0.4, 0.5, 1,  5]\n",
    "}\n",
    "svm_new = GridSearchCV(svm, parameters, cv=5)\n",
    "svm_new.fit(X_features, y)\n",
    "print(f\"Best score for SVM {svm_new.best_score_}\")\n",
    "print(f\"Best params for SVM {svm_new.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример 1: использование полиномиальных характеристик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=100, noise=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "clfs = [RandomForestClassifier(), \n",
    "        LogisticRegression(),\n",
    "        SVC(), \n",
    "        KNeighborsClassifier()]\n",
    "        \n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    ncols=(len(clfs) + 1)// 2,\n",
    "    nrows=2, figsize=(6 * ((len(clfs) + 1)) // 2, 12),\n",
    "    dpi=75\n",
    ")\n",
    "\n",
    "labels = [ 'RandomForestClassifier', 'LogisticRegression',\n",
    "         'SVM with RBF kernel', 'kNN']\n",
    "\n",
    "for clf, ax, label in zip(clfs, axes.flat, labels):\n",
    "    clf.fit(X, y)\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=1, ax=ax)\n",
    "    accuracy = clf.score(X, y)\n",
    "    ax.set_title(label + ', accuracy = ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "polyTransform = PolynomialFeatures(degree=2)\n",
    "scaler = StandardScaler()\n",
    "X_poly = polyTransform.fit_transform(X)\n",
    "X_poly = scaler.fit_transform(X_poly)\n",
    "\n",
    "clfTree = Pipeline([('polyTransform', PolynomialFeatures(degree=2)), \n",
    "                     ('scaler', scaler),\n",
    "                ('decision_tree', DecisionTreeClassifier(max_depth=4))])\n",
    "\n",
    "clfForest = Pipeline([('polyTransform', PolynomialFeatures(degree=2)), \n",
    "                     ('scaler', scaler),\n",
    "                ('random_forest', RandomForestClassifier(n_estimators=25, max_depth=3))])\n",
    "\n",
    "clfs = [clfTree, clfForest]\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    ncols=2, nrows=1, figsize=(18, 8),\n",
    "    dpi=75\n",
    ")\n",
    "\n",
    "labels = ['DecisionTreeClassifier', 'RandomForestClassifier']\n",
    "\n",
    "for clf, ax, label in zip(clfs, axes.flat, labels):\n",
    "    clf.fit(X, y)\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=1, ax=ax)\n",
    "    accuracy = clf.score(X, y)\n",
    "    ax.set_title(label + ', accuracy = ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример 2: Мультикласс классификация для конкурса предсказания состава стекла"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные: https://www.kaggle.com/uciml/glass#glass.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/glass.csv')\n",
    "X, y = data.drop('Type', axis=1), data.Type\n",
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "\n",
    "clfs = [DecisionTreeClassifier(max_depth = 7), \n",
    "        RandomForestClassifier(n_estimators=100, max_depth=3),\n",
    "        KNeighborsClassifier(n_neighbors=5),\n",
    "        LogisticRegression()]\n",
    "\n",
    "names = ['Decision Tree', 'Random Forest', 'KNN', 'Logistic regression']\n",
    "\n",
    "Scores = pd.DataFrame({'method':['Multiclass', 'OneVsRest', 'OneVsOne']})\n",
    "Times = pd.DataFrame({'method':['Multiclass', 'OneVsRest', 'OneVsOne']})\n",
    "\n",
    "for clf, name in zip(clfs, names):\n",
    "    scores = []\n",
    "    times = []\n",
    "    start = time.time() #отсчет времени\n",
    "    score = cross_val_score(estimator=clf, X=X, y=y, scoring='f1_macro', cv=5).mean()\n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "    scores.append(score)\n",
    "    \n",
    "    clf = OneVsRestClassifier(clf)\n",
    "    start = time.time()\n",
    "    score = cross_val_score(estimator=clf, X=X, y=y, scoring='f1_macro', cv=5).mean()\n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "    scores.append(score)\n",
    "    \n",
    "    clf = OneVsOneClassifier(clf)\n",
    "    start = time.time()\n",
    "    score = cross_val_score(estimator=clf, X=X, y=y, scoring='f1_macro', cv=5).mean()\n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "    scores.append(score)\n",
    "    \n",
    "    Scores[name] = scores\n",
    "    Times[name] = times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Посмотрим на скоры лучших моделей. Как расположены предсказания классов: ближе к [0,1]  или около 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search для мультиклассовой классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {# <YOUR CODE>}\n",
    "\n",
    "dtc= # <YOUR CODE>\n",
    "dtc_new = GridSearchCV(dtc, parameters, cv=5)\n",
    "dtc_new.fit(X, y)\n",
    "print(f\"Best score for dtc {dtc_new.best_score_}\")\n",
    "print(f\"Best params for dtc {dtc_new.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyTransform = PolynomialFeatures(degree=2)\n",
    "scaler = StandardScaler()\n",
    "X_poly = polyTransform.fit_transform(X)\n",
    "X_poly = scaler.fit_transform(X_poly)\n",
    "\n",
    "clfTree = Pipeline([('polyTransform', PolynomialFeatures(degree=3)), \n",
    "                     ('scaler', scaler),\n",
    "                    ('dim_red', SelectKBest(k=6)),\n",
    "                ('decision_tree', DecisionTreeClassifier(max_depth=4))])\n",
    "\n",
    "clfForest = Pipeline([('polyTransform', PolynomialFeatures(degree=3)), \n",
    "                     ('scaler', scaler),\n",
    "                      ('dim_red', SelectKBest(k=6)),\n",
    "                ('random_forest', RandomForestClassifier(n_estimators=25, max_depth=3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [clfTree, clfForest ]\n",
    "\n",
    "names = ['Decision Tree', 'Random Forest', 'KNN', 'Logistic regression']\n",
    "\n",
    "Scores = pd.DataFrame({'method':['Multiclass', 'OneVsRest', 'OneVsOne']})\n",
    "Times = pd.DataFrame({'method':['Multiclass', 'OneVsRest', 'OneVsOne']})\n",
    "\n",
    "for clf, name in zip(clfs, names):\n",
    "    scores = []\n",
    "    times = []\n",
    "    start = time.time() #отсчет времени\n",
    "    score = cross_val_score(estimator=clf, X=X, y=y, scoring='f1_macro', cv=5).mean()\n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "    scores.append(score)\n",
    "    \n",
    "    clf = OneVsRestClassifier(clf)\n",
    "    start = time.time()\n",
    "    score = cross_val_score(estimator=clf, X=X, y=y, scoring='f1_macro', cv=5).mean()\n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "    scores.append(score)\n",
    "    \n",
    "    clf = OneVsOneClassifier(clf)\n",
    "    start = time.time()\n",
    "    score = cross_val_score(estimator=clf, X=X, y=y, scoring='f1_macro', cv=5).mean()\n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "    scores.append(score)\n",
    "    \n",
    "    Scores[name] = scores\n",
    "    Times[name] = times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отбор признаков:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Мы можем проводить отбор признаков:\n",
    "- на основе модели\n",
    "- статистическими тестами \n",
    "- регуляризацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select= SelectKBest(k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X[X.columns[select.get_support()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "select.scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Попробуем реализовать это в пайплайне:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, SelectFromModel\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "cancer= load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([('selectk', SelectKBest(k=6)), \n",
    "                     ('scaler', StandardScaler()),\n",
    "                ('decision_tree',  RandomForestClassifier(n_estimators=25, max_depth=3))])\n",
    "\n",
    "cross_val_score(estimator=clf, X=X_train, y=y_train, cv=5).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
